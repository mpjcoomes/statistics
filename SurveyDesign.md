*Survey Design*
===============

-   Introduction
    -   1.1 Aims
    -   1.2 Rationale
    -   1.3 Objectives
    -   1.4 Research Questions
    -   1.5 Research Design
-   Method
    -   2.1 Target Population
    -   2.2 Sampling Frame
    -   2.3 Sampling Procedure
    -   2.4 Sample Criticisms
-   Information Areas & Questionnaire
    -   3.1 Information Areas
    -   3.2 Questionnaire Structure
    -   3.3 Questionnaire Development
-   Data Collection
    -   4.1 Collection
    -   4.2 Ethics
-   Data Analysis Plan
    -   5.1 Data preparation
    -   5.2 Research Question Analysis
-   Conclusion
-   References
    -   Acknowledgements
-   Appendix. Item Codebook and Analysis



Introduction
============

1.1 Aims
--------

This study aims to describe whether attitudes and behaviours of students support working in China. The China National Tourism Administration (CNTA) wants to attract Australian tertiary students to China for graduate employment and has commissioned this preliminary survey. The survey also aims to inform future research and marketing campaigns by collecting qualitative information.

1.2 Rationale
-------------

This pilot study measures attitudes and behaviours to establish a baseline for these factors in the population. Qualitative feedback enhances these baselines by providing more in depth information on the reasons for participant responses. To keep the amount of qualitative data manageable it will only be collected from extreme scores, which although not be representative of the population, will nevertheless contribute to research aims.

1.3 Objectives
--------------

Of the four research objectives, the first is to select a representative sample of students across all major tertiary faculties. The second is to survey attitudes towards China as a place of employment. The third is to survey the extent of behaviours that relate to employment in China. The final objective is to inform these results with qualitative data from extreme scores.

1.4 Research Questions
----------------------

Of the four research questions, the first is whether population attitudes towards working in China are positive or negative. The second is whether population behaviours support or oppose employment in China. The third is whether there is any difference between participant attitude or behaviour scores across faculties. The fourth research question is what reasons participants with extreme scores report for their responses.

The results of the first two questions will be generalised to the population, while the results of the last two will be less generalisable. For question three, this is because dividing the total sample by faculty will result in small and unequal sample sizes between groups. For question four, because limiting the sample to those with extreme scores creates systemic bias.

1.5 Research Design
-------------------

This study will be observational in design, with students stratified by faculty and randomly selected to complete a Computer Assisted Self-Interview (De Vaus, 2002). Group-wise comparisons and qualitative data collection are secondary, with the primary functions of the design being to survey and describe.

Method
======

2.1 Target Population
---------------------

The target population is 2011 students (N\~18,517; Faux University of Technology, 2009) of any degree type and academic load who have continued their enrolment by the semester one withdrawal date. As the research aim relates to employment, the target population includes students from all faculties, not just those for whom it is convenient to participate.

2.2 Sampling Frame
------------------

Students will be stratified by the six main faculties. Stratification will maximise sample representativeness, specifically in terms of professions trained at different faculties. This process reduces non-response bias in specific professional streams.

A list of valid student emails will be collected from the student database in connection with university management. This list of emails should encompass the entire target population and most emails will be active and functional at the time of collection. This method will produce an accurate and high quality sampling frame.

2.3 Sampling Procedure
----------------------

The email list will first be stratified by faculty, most likely by extracting emails from the database pre-coded with each student's home faculty. Alternatively, individual pre-stratified sampling frames could be collected from each faculty of that faculty's students only. Either way, once the sampling frames are prepared, a random numbers table will be used to randomly sample emails from each frame.

The agreed sample size was approximately 400 students. The n of individual faculty samples will be proportionate to the number of students studying within that faculty. For example, if the Design faculty represents 11% of the student population, then it will represent 11% of the 400 students (*n*=44).

Each randomly selected participant will be sent an email offer to participate, containing the cover letter complete with URL link directing recipients to a web page hosted by CNTA. The web page contains another copy of the cover letter and the survey. To prevent multiple submissions by the same participants, each will be required to enter their email for cross-referencing with the sample list before accessing the survey.

2.4 Sample Criticisms
---------------------

Although more participants would reduce standard error, 400 was the largest sample size would agree to and the \$2000 CNTA budget allowed. This sample size is sufficient to meet the research aims and at least the first two research questions.

When divided across faculties for the third research question, the sample sizes across groups will decrease and the standard error will increase. The sample will be less representative and results, if statistically significant and valid, will need to be interpreted cautiously. Sample size is less relevant to research question four as the design already compromises population representativeness (i.e. only participants with extreme scores will be measured). However, the sample used should still meet the narrow goals of that question.

A significant criticism of this sampling design is the potential for non-response bias. A reward to encourage participation may correct this shortcoming. Therefore, entry into a lottery for three Bookshop vouchers valued at \$100 each will be advertised in the cover letter as a reward for participation. Furthermore, non-responders will be emailed a second time two weeks after the initial mail out. This process will be repeated with additional rounds of random sampling and mail outs until the required number of participants is obtained from each strata.

Information Areas & Questionnaire
=================================

3.1 Information Areas
---------------------

The Chinese Employment Attitudes and Behaviours Scale (CEABS) uses multiple item styles to measure attributes, attitudes, behaviours and qualitative reasons.

Each of these information areas corresponds to a research objective. The first information area, attributes, meets the first objective by assessing the sample's population representativeness. The second information area, attitudes towards China as a place of employment, meets the second objective. The third information area, China related employment behaviours, meets the third research objective. The fourth information area qualitatively measures reasons for extreme scores to inform other results and meets the fourth research objective.

3.2 Questionnaire Structure
---------------------------

The questionnaire has three sections and the first three information areas each have an independent section. The fourth information area is located at the end of both the attitude and behaviour sections.

Attributes are measured first in section one with simple questions and item designs to facilitate ease of entry into the survey. Sex is measured with a dichotomous response item, age with a numerical open response item, and both faculty and country of birth (COB) with multiple nominal response items. Countries available includes those most common for students collapsed by region with an additional 'other' option.

Section two measures attitudes with five items on a Likert scale coded 0 to 4. A total score of 0, 10 or 20 indicates negative, neutral or positive attitudes respectively.

Section three measures behaviours with a one-item scale in checklist response format. Each checklist option is an activity in favour of employment in China and participants select as many options as appropriate. Each counts as 1, summed to a total score that indicates how many of the behaviours participants performed in the last four years.

If participants have extreme scores on either the second or third section a text box appears for that section prompting them for open-response text entry. Extreme scores are defined as outside 5 to 15 for section two and outside 2 to 8 for section three. This item piping feature is only available when in electronic format. An item codebook and analysis summary are tabled in the Appendix.

3.3 Questionnaire Development
-----------------------------

The draft cover letter and questionnaire was reviewed by a third party before being finalised. The third party was chosen based on their tourist experiences there and their employment in an engineering company with contracts in China.

They were emailed the draft, made tracked changes and added comments. The researcher reviewed the feedback, made changes to the draft, and followed up with the reviewer on those changes. The feedback was condensed and is presented in Table 1.

Table 1. *Condensed Reviewer Feedback*

|**Question**|Response|
|------------|--------|
|*What items did reviewers have difficulty with? Why?*|- The open-response section for Q10: "What information is aimed to be gained by this particular box? Perhaps this box should only appear on very high scores?" The reviewer thought most people would select zero or very few boxes and a better approach would be to only take text from high scores. Based on this feedback, the definition of extreme (outside 3-7) was narrowed to scores outside the range of 2-8.|
|*How could the wording, layout, order etc. of these items be improved?*|- Section titles could be softened (e.g. "Attributes" could be "information about you"). The titles were softened in response.|
|-|- Q3: "Chinese born people are collapsed with other Asian regions, biasing the results." To correct this, a 'Mainland China' options was added, allowing researchers to screen and resample as needed to avoid bias from native Chinese participants. A restriction was also added to the cover letter on this point.|
|-|- Q6: Too many possible interpretations (e.g. "unstable in the sense that the job might only last a few months or unstable in the sense of political unrest"). The item wasn't changed because it is meant to be general. Overly specific items lose information (e.g. from people who do think employment is unstable but for different reasons) and distort response distributions.|
|-|- Q8 and Q9: These items interact, with Q8 (negatively worded) possibly lowering responses to Q9 (positively worded). Normally this cannot be avoided short of scrapping all reverse coded items. However, in this case the content of items could interact. I resolved the concern by simply swapping the two items.|
|-|- Q10: 'Found out Chinese Tax Rates' should be 'enquired' or 'investigated'. Item amended to 'investigated'.|
|-|- "Q10: sounds very formal and may distract potential candidates". Item amended to "In the past 4 years have you".|
|*Are there any other questions that should be included?*|- "The questionnaire could be affected by the type of job people are thinking about. But arguably they may be looking at jobs following on from the faculty they are enrolled in." The researcher agrees, and this is exactly why the sample is stratified by faculty and research question three compares across faculties.|
|*Overall, how could the design of the questionnaire be improved?*|- "The cover letter doesn't say why they want to measure attitudes and behaviours". A passage was introduced to correct this.|

Data Collection
===============

4.1 Collection
--------------

The first collection method is that participants follow the link provided in the email and complete the survey online. An alternative collection method is a printed paper format.

The first method is superior as data can be automatically collected, coded and exported for later analysis. This streamlines the research process, requires less time from participants and reduces manual entry errors. The second method is inferior as it lacks these advantages and would necessitate alterations to the scoring style. The qualitative component in particular would require instructions on self-scoring in order to capture extreme scores only.

4.2 Ethics
----------

There are few ethical concerns because the survey is restricted to professional topics and the questions are relatively impersonal. However, some participants may perceive such topics as sensitive or may volunteer sensitive information in their text responses. Therefore, the cover letter will explain confidentiality, only the primary researcher will have access to identifiers and these identifiers will be removed from the dataset prior to archiving.

Any invasion of privacy is minor. Although email contact may be considered an invasion of privacy, most participants would experience it every normal day in the form of email spam. Furthermore, email is arguably less invasive than phone surveys, which are an accepted and normal component of survey research.

Data Analysis Plan
==================

5.1 Data preparation
--------------------

The first process will be a preparation and exploration of the sample quality. Frequencies will confirm the size of each stratum and the data will be cleaned of errors, mainland Chinese born participants and missing values. Then the distributions of age, sex, COB and scale scores will be explored.

5.2 Research Question Analysis
------------------------------

After data preparation all analyses will follow the four research questions:

1.  *Whether attitudes towards working in China are positive or negative*: Attitude items will be summed into a scale giving a single score for each participant. The overall mean, standard deviation and a 95% confidence interval will answer the research question. Attitude scores will be interpreted as specified in section 3.2 (i.e. 0=negative; 20=positive).
2.  *Whether population behaviours support or oppose employment in China*: As above, a mean, standard deviation and 95% confidence interval will answer this question. Although there is no baseline for comparison, the item is designed so raw scores can be interpreted as the number of employment behaviours (e.g. mean 1.6=on average the population engaged in 1.6 pro-China employment behaviours over the past four years).
3.  *Whether there is any difference between participant attitude or behaviour scores across faculties*: For attitudes, researchers will calculate descriptive statistics and produce a graph of boxplots with a separate boxplot representing each faculty's attitude scores. After that broad overview, a one-way ANOVA will compare means across faculties for significant differences. An identical analysis will be carried out for behaviour scores. Given the unequal group sizes produced by the stratification design particular emphasis will be placed on assessing the homogeneity of variance assumption. Analysis will finish with a larger crosstabulation of mean scores with attitude and behaviour as rows, faculty as columns.
4.  *What reasons did participants with extreme scores report for their responses*: Basic word frequencies will determine common meaningful words. Researchers will review all text subjectively and add key statements to the final report. While the sample is made up of extreme scores and cannot be generalised it will still help explain why participants answered the way they did and be useful for future research. Analyses are summarised in the Appendix.

Conclusion
==========

This project should provide a representative sample, survey attitudes and behaviours towards China as a place of employment, and inform those results with qualitative feedback.

The write up and review processes required more time than expected. For the write up, while authors have firm ideas about how the study, sampling and questionnaire will be structured, it is time consuming to clearly articulate these ideas in a report. For the review process, it was unexpected how tedious it would be going back and forth with the reviewer, clarifying different sections and discussing improvements.

There were three lessons about survey design learnt from this project. Firstly, an author's writing style and unknown biases are reflected in the questionnaires they write. Certain wording, items and layouts can seem clear to authors when in reality they are unclear to participants. Secondly, a third party review corrects many of these shortcomings. The final lesson is that setting out clear research objectives and questions at the beginning of a project makes planning the design, data collection and analysis process much simpler.

References
==========

De Vaus, D.A. (2002). Surveys in social research. 5th Edition, Sydney: Allen & Unwin.

Appendix. Item Codebook and Analysis
======================================

|Item|Values|Analysis|
|----|------|--------|
|sex|0 male 1 female|Initial data exploration of representativeness, frequencies and distribution.|
|age|Participant entered numeral|See sex.|
|cob|0 Australia, New Zealand or other Oceania region 1 United States of America or other Americas region 2 Africa or Middle East region 3 Mainland China 4 Hong Kong or Asia region 5 UK and European region 6 Other|See sex. Will be used to ensure no Mainland China born participants have biased the sample.|
|faculty|0 Business & Enterprise 1 Design 2 Engineering & Industrial Sciences 3 Higher Education Lilydale 4 Information & Communication Technologies 5 Life & Social Sciences|Initial data exploration of representativeness. One-way ANOVA for differences between attitudes across faculties. One-way ANOVA for differences between behaviours across faculties. Crosstabulations with attitude and behaviour scale scores as rows, faculty as columns. Used as the label variable in boxplots (e.g. each plot representing a faculty).|
|Q5\_atts|0 Strongly Disagree 1 Disagree 2 Neither agree nor disagree 3 Agree 4 Strongly Agree|Likert Scale. This item will be summed with the other four attitude items to produce a scale for analysis. The distribution and scale score will be explored for the first three research questions.|
|Q6\_atts|4 Strongly Disagree 3 Disagree 2 Neither agree nor disagree 1 Agree 0 Strongly Agree|Reverse coded. See Q5\_atts.|
|Q7\_atts|0 Strongly Disagree 1 Disagree 2 Neither agree nor disagree 3 Agree 4 Strongly Agree|See Q5\_atts.|
|Q8\_atts|0 Strongly Disagree 1 Disagree 2 Neither agree nor disagree 3 Agree 4 Strongly Agree|See Q5\_atts.|
|Q9\_atts|4 Strongly Disagree 3 Disagree 2 Neither agree nor disagree 1 Agree 0 Strongly Agree|Reverse coded. See Q5\_atts.|
|Q10\_behs|0 or 1 Researched living in China 0 or 1 Travelled to China 0 or 1 Looked into Visa applications 0 or 1 Learned some Chinese words 0 or 1 Investigated Chinese tax rates 0 or 1 Researched working in China 0 or 1 Made enquiries about Chinese cities 0 or 1 Checked my passport was valid 0 or 1 Found news articles on China's job market 0 or 1 Created an account at a Chinese bank|All items selected are summed to create a single item scale for behaviours.|
|Qu\_atts|Open-response text on the reasons for extreme attitudes.|Basic word frequencies will be extracted from the qualitative data. Researchers will subjectively review all text entered for patterns of reasoning. Some passages will be added to the final report.|
|Qu\_behs|Open-response text on the reasons for extreme attitudes.|See Qu\_atts.|
